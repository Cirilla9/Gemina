diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/include/linux/huge_mm.h Ingens-5.10-cow/include/linux/huge_mm.h
--- linux-5.10/include/linux/huge_mm.h	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/include/linux/huge_mm.h	2025-03-23 07:51:06.794189175 +0000
@@ -97,6 +97,17 @@
 #endif
 };
 
+enum page_check_address_pmd_flag {
+	PAGE_CHECK_ADDRESS_PMD_FLAG,
+	PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG,
+	PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG,
+};
+extern pmd_t *page_check_address_pmd(struct page *page,
+				     struct mm_struct *mm,
+				     unsigned long address,
+				     enum page_check_address_pmd_flag flag,
+				     spinlock_t **ptl);
+					 
 struct kobject;
 struct kobj_attribute;
 
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/include/linux/ksm.h Ingens-5.10-cow/include/linux/ksm.h
--- linux-5.10/include/linux/ksm.h	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/include/linux/ksm.h	2025-03-23 07:51:07.061201017 +0000
@@ -15,6 +15,11 @@
 #include <linux/sched.h>
 #include <linux/sched/coredump.h>
 
+//zhehua
+extern unsigned long nr_ksm_cows;
+extern unsigned long nr_ksm_shares;
+extern unsigned int runksm;
+
 struct stable_node;
 struct mem_cgroup;
 
@@ -23,6 +28,8 @@
 		unsigned long end, int advice, unsigned long *vm_flags);
 int __ksm_enter(struct mm_struct *mm);
 void __ksm_exit(struct mm_struct *mm);
+int unmerge_ksm_pages(struct vm_area_struct *vma,
+			     unsigned long start, unsigned long end);
 
 static inline int ksm_fork(struct mm_struct *mm, struct mm_struct *oldmm)
 {
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/include/linux/mm.h Ingens-5.10-cow/include/linux/mm.h
--- linux-5.10/include/linux/mm.h	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/include/linux/mm.h	2025-03-23 07:51:06.422754034 +0000
@@ -3143,10 +3143,7 @@
 
 extern int memcmp_pages(struct page *page1, struct page *page2);
 
-static inline int pages_identical(struct page *page1, struct page *page2)
-{
-	return !memcmp_pages(page1, page2);
-}
+extern int pages_identical(struct page *page1, struct page *page2);
 
 #ifdef CONFIG_MAPPING_DIRTY_HELPERS
 unsigned long clean_record_shared_mapping_range(struct address_space *mapping,
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/include/linux/mm_types.h Ingens-5.10-cow/include/linux/mm_types.h
--- linux-5.10/include/linux/mm_types.h	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/include/linux/mm_types.h	2025-03-23 07:51:07.498191454 +0000
@@ -371,6 +371,9 @@
 	struct mempolicy *vm_policy;	/* NUMA policy for the VMA */
 #endif
 	struct vm_userfaultfd_ctx vm_userfaultfd_ctx;
+	unsigned long cows;
+	unsigned long ksms;
+	unsigned long nones;
 } __randomize_layout;
 
 struct core_thread {
@@ -557,6 +560,9 @@
 		u32 pasid;
 #endif
 	} __randomize_layout;
+	unsigned long cows;
+	unsigned long ksms;
+	unsigned long nones;
 
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/Makefile Ingens-5.10-cow/Makefile
--- linux-5.10/Makefile	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/Makefile	2025-03-23 07:51:07.784288070 +0000
@@ -2,7 +2,7 @@
 VERSION = 5
 PATCHLEVEL = 10
 SUBLEVEL = 0
-EXTRAVERSION =
+EXTRAVERSION = -Ingens-cow
 NAME = Kleptomaniac Octopus
 
 # *DOCUMENTATION*
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/mm/huge_memory.c Ingens-5.10-cow/mm/huge_memory.c
--- linux-5.10/mm/huge_memory.c	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/mm/huge_memory.c	2025-03-23 07:51:07.885124263 +0000
@@ -12,6 +12,7 @@
 #include <linux/highmem.h>
 #include <linux/hugetlb.h>
 #include <linux/mmu_notifier.h>
+#include <linux/pagemap.h>
 #include <linux/rmap.h>
 #include <linux/swap.h>
 #include <linux/shrinker.h>
@@ -37,7 +38,9 @@
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
 #include "internal.h"
-
+#include <linux/rmap.h>
+#include <linux/mmu_notifier.h>
+#include <linux/init.h>
 /*
  * By default, transparent hugepage support is disabled in order to avoid
  * risking an increased memory footprint for applications that are not
@@ -2410,6 +2413,62 @@
 	lru_add_page_tail(head, page_tail, lruvec, list);
 }
 
+/*
+ * This function returns whether a given @page is mapped onto the @address
+ * in the virtual space of @mm.
+ *
+ * When it's true, this function returns *pmd with holding the page table lock
+ * and passing it back to the caller via @ptl.
+ * If it's false, returns NULL without holding the page table lock.
+ */
+pmd_t *page_check_address_pmd(struct page *page,
+			      struct mm_struct *mm,
+			      unsigned long address,
+			      enum page_check_address_pmd_flag flag,
+			      spinlock_t **ptl)
+{
+	pgd_t *pgd;
+	p4d_t *p4d;
+	pud_t *pud;
+	pmd_t *pmd = NULL;
+
+	if (address & ~HPAGE_PMD_MASK)
+		return NULL;
+
+	pgd = pgd_offset(mm, address);
+	if (!pgd_present(*pgd))
+		return NULL;
+	p4d = p4d_offset(pgd, address);
+	if (!p4d_present(*p4d))
+		return NULL;
+	pud = pud_offset(p4d, address);
+	if (!pud_present(*pud))
+		return NULL;
+	pmd = pmd_offset(pud, address);
+
+	*ptl = pmd_lock(mm, pmd);
+	if (!pmd_present(*pmd))
+		goto unlock;
+	if (pmd_page(*pmd) != page)
+		goto unlock;
+	/*
+	 * split_vma() may create temporary aliased mappings. There is
+	 * no risk as long as all huge pmd are found and have their
+	 * splitting bit set before __split_huge_page_refcount
+	 * runs. Finding the same huge pmd more than once during the
+	 * same rmap walk is not a problem.
+	 */
+	if (flag == PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG)
+		goto unlock;
+	if (pmd_trans_huge(*pmd)) {
+		VM_BUG_ON(flag == PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG);
+		return pmd;
+	}
+unlock:
+	spin_unlock(*ptl);
+	return NULL;
+}
+
 static void __split_huge_page(struct page *page, struct list_head *list,
 		pgoff_t end, unsigned long flags)
 {
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/mm/khugepaged.c Ingens-5.10-cow/mm/khugepaged.c
--- linux-5.10/mm/khugepaged.c	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/mm/khugepaged.c	2025-03-23 07:51:07.834266057 +0000
@@ -18,6 +18,7 @@
 #include <linux/page_idle.h>
 #include <linux/swapops.h>
 #include <linux/shmem_fs.h>
+#include <linux/ksm.h>
 
 #include <asm/tlb.h>
 #include <asm/pgalloc.h>
@@ -1233,6 +1234,7 @@
 	}
 
 	memset(khugepaged_node_load, 0, sizeof(khugepaged_node_load));
+	// unmerge_ksm_pages(vma, address, address+511*PAGE_SIZE);
 	pte = pte_offset_map_lock(mm, pmd, address, &ptl);
 	for (_address = address, _pte = pte; _pte < pte+HPAGE_PMD_NR;
 	     _pte++, _address += PAGE_SIZE) {
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/mm/ksm.c Ingens-5.10-cow/mm/ksm.c
--- linux-5.10/mm/ksm.c	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/mm/ksm.c	2025-03-23 07:51:07.822219487 +0000
@@ -13,6 +13,7 @@
  *	Hugh Dickins
  */
 
+#include "linux/page-flags.h"
 #include <linux/errno.h>
 #include <linux/mm.h>
 #include <linux/fs.h>
@@ -50,6 +51,9 @@
 #define DO_NUMA(x)	do { } while (0)
 #endif
 
+#define ksm_printk(fmt, ...) \
+	do { if(ksm_debug) trace_printk(fmt, __VA_ARGS__); } while(0)
+
 /**
  * DOC: Overview
  *
@@ -164,6 +168,7 @@
 		};
 	};
 	struct hlist_head hlist;
+	uint8_t is_huge_kpage;
 	union {
 		unsigned long kpfn;
 		unsigned long chain_prune_time;
@@ -203,6 +208,7 @@
 	struct mm_struct *mm;
 	unsigned long address;		/* + low bits used for flags below */
 	unsigned int oldchecksum;	/* when unstable */
+	int8_t is_huge_page;
 	union {
 		struct rb_node node;	/* when node of unstable tree */
 		struct {		/* when listed from stable tree */
@@ -217,6 +223,12 @@
 #define STABLE_FLAG	0x200	/* is listed from the stable tree */
 #define KSM_FLAG_MASK	(SEQNR_MASK|UNSTABLE_FLAG|STABLE_FLAG)
 				/* to mask all the flags */
+#define NO_HPAGE_MERGE 0
+#define HPAGE_MERGE_SPLIT 1
+#define HPAGE_MERGE_NO_SPLIT 2
+#define HPAGE_MERGE_PRESERVE 3
+#define HPAGE_MERGE_FREQUENCY 4
+#define HPAGE_MERGE_MAX 4
 
 /* The stable and unstable tree heads */
 static struct rb_root one_stable_tree[1] = { RB_ROOT };
@@ -224,6 +236,14 @@
 static struct rb_root *root_stable_tree = one_stable_tree;
 static struct rb_root *root_unstable_tree = one_unstable_tree;
 
+#if 1
+static struct rb_root root_stable_tree_huge[1];
+static struct rb_root root_unstable_tree_huge[1];
+#else
+static struct rb_root *root_stable_tree_huge = one_stable_tree;
+static struct rb_root *root_unstable_tree_huge = one_unstable_tree;
+#endif
+
 /* Recently migrated nodes of stable tree, pending proper placement */
 static LIST_HEAD(migrate_nodes);
 #define STABLE_NODE_DUP_HEAD ((struct list_head *)&migrate_nodes.prev)
@@ -242,11 +262,18 @@
 static struct kmem_cache *stable_node_cache;
 static struct kmem_cache *mm_slot_cache;
 
+//zhehua
+unsigned long nr_ksm_cows = 0;
+unsigned long nr_ksm_shares = 0;
+unsigned int runksm = 0;
+
 /* The number of nodes in the stable tree */
 static unsigned long ksm_pages_shared;
 
 /* The number of page slots additionally sharing those nodes */
 static unsigned long ksm_pages_sharing;
+static unsigned long ksm_pages_sharing_huge;
+static unsigned long ksm_pages_sharing_huge1;
 
 /* The number of nodes in the unstable tree */
 static unsigned long ksm_pages_unshared;
@@ -271,6 +298,18 @@
 
 /* Milliseconds ksmd should sleep between batches */
 static unsigned int ksm_thread_sleep_millisecs = 20;
+static unsigned int adaptive_sleep_interval = 20;
+static unsigned int sleep_scaling_factor = 10;
+static unsigned int hpage_preserve = 70; /* 70% */
+
+static unsigned int ksm_debug;
+static unsigned int ksm_docmp=1;
+static unsigned int ksm_merge_hugepage = HPAGE_MERGE_FREQUENCY;
+
+/* do not use kernel crypto.
+ * KSMd holds lock(rwsem) at non-sleeping context
+ * but crypto functions are sleeping functions */
+#undef USE_JHASH
 
 /* Checksum of an empty (zeroed) page */
 static unsigned int zero_checksum __read_mostly;
@@ -380,13 +419,18 @@
 
 	rmap_item = kmem_cache_zalloc(rmap_item_cache, GFP_KERNEL |
 						__GFP_NORETRY | __GFP_NOWARN);
+	/*
 	if (rmap_item)
 		ksm_rmap_items++;
+	*/
 	return rmap_item;
 }
 
 static inline void free_rmap_item(struct rmap_item *rmap_item)
 {
+	if (rmap_item->is_huge_page > 0)
+		ksm_rmap_items -= HPAGE_PMD_NR;
+	else
 	ksm_rmap_items--;
 	rmap_item->mm = NULL;	/* debug safety */
 	kmem_cache_free(rmap_item_cache, rmap_item);
@@ -452,6 +496,29 @@
 	return atomic_read(&mm->mm_users) == 0;
 }
 
+#ifdef USE_JHASH
+static inline u32 get_page_hash_digest(struct page *page)
+{
+	void *addr;
+	u32 checksum = 17;
+	if (PageTransCompound(page)) {
+		int i;
+
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			addr = kmap_atomic(page + i);
+			checksum = jhash2(addr, PAGE_SIZE / 4, checksum);
+			kunmap_atomic(addr);
+		}
+
+	} else {
+		addr = kmap_atomic(page);
+		checksum = jhash2(addr, PAGE_SIZE / 4, checksum);
+		kunmap_atomic(addr);
+	}
+	return checksum;
+}
+#endif
+
 /*
  * We use break_ksm to break COW on a ksm page: it's a stripped down
  *
@@ -470,6 +537,7 @@
 static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 {
 	struct page *page;
+	int trial = 0;
 	vm_fault_t ret = 0;
 
 	do {
@@ -485,6 +553,16 @@
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
+
+		/* FIXME: Does khugepaged set write bit, making handle_mm_fault returns 0? */
+		if (PageTransCompound(page)) {
+			if (ret == 0) {
+				trace_printk("handle_mm_fault returns 0.. retrying %d\n", trial);
+				if (trial > 100)
+					break;
+			}
+		}
+		trial++;
 	} while (!(ret & (VM_FAULT_WRITE | VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop because handle_mm_fault() may back out if there's
@@ -550,6 +628,20 @@
 	mmap_read_unlock(mm);
 }
 
+static struct page *page_trans_compound_anon(struct page *page)
+{
+	if (PageTransCompound(page)) {
+		struct page *head = compound_head(page);
+		/*
+		 * head may actually be splitted and freed from under
+		 * us but it's ok here.
+		 */
+		if (PageAnon(head))
+			return head;
+	}
+	return NULL;
+}
+
 static struct page *get_mergeable_page(struct rmap_item *rmap_item)
 {
 	struct mm_struct *mm = rmap_item->mm;
@@ -632,15 +724,27 @@
 static void remove_node_from_stable_tree(struct stable_node *stable_node)
 {
 	struct rmap_item *rmap_item;
-
+	struct vm_area_struct *vma;
 	/* check it's not STABLE_NODE_CHAIN or negative */
 	BUG_ON(stable_node->rmap_hlist_len < 0);
 
 	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
-		if (rmap_item->hlist.next)
+		if (rmap_item->hlist.next){
+			if (stable_node->is_huge_kpage > 0) {
+				ksm_pages_sharing -= HPAGE_PMD_NR;
+				ksm_pages_sharing_huge -= HPAGE_PMD_NR;
+			} else
 			ksm_pages_sharing--;
+		} else {
+			if (stable_node->is_huge_kpage > 0)
+				ksm_pages_shared -= HPAGE_PMD_NR;
 		else
 			ksm_pages_shared--;
+		}
+		vma = find_mergeable_vma(rmap_item->mm, rmap_item->address);
+		if (vma) {
+			vma->ksms++;
+		}
 		VM_BUG_ON(stable_node->rmap_hlist_len <= 0);
 		stable_node->rmap_hlist_len--;
 		put_anon_vma(rmap_item->anon_vma);
@@ -662,8 +766,13 @@
 
 	if (stable_node->head == &migrate_nodes)
 		list_del(&stable_node->list);
+	else {
+		if (stable_node->is_huge_kpage)
+			rb_erase(&stable_node->node,
+				 root_stable_tree_huge + NUMA(stable_node->nid));
 	else
 		stable_node_dup_del(stable_node);
+	}
 	free_stable_node(stable_node);
 }
 
@@ -707,6 +816,15 @@
 	if (READ_ONCE(page->mapping) != expected_mapping)
 		goto stale;
 
+	/* Ksm page was huge page when it is inserted into stable tree
+	 * but now it is split into base pages */
+	if (stable_node->is_huge_kpage) {
+		if (!PageTransCompound(page))
+			goto stale;
+	} else {
+		if (PageTransCompound(page))
+			goto stale;
+	}
 	/*
 	 * We cannot do anything with the page while its refcount is 0.
 	 * Usually 0 means free, or tail of a higher-order page: in which
@@ -786,10 +904,18 @@
 		unlock_page(page);
 		put_page(page);
 
-		if (!hlist_empty(&stable_node->hlist))
+		if (!hlist_empty(&stable_node->hlist)){
+			if (stable_node->is_huge_kpage > 0) {
+				ksm_pages_sharing -= HPAGE_PMD_NR;
+				ksm_pages_sharing_huge -= HPAGE_PMD_NR;
+			} else
 			ksm_pages_sharing--;
+		}else{
+			if (stable_node->is_huge_kpage > 0)
+				ksm_pages_shared -= HPAGE_PMD_NR;
 		else
 			ksm_pages_shared--;
+		}
 		VM_BUG_ON(stable_node->rmap_hlist_len <= 0);
 		stable_node->rmap_hlist_len--;
 
@@ -806,10 +932,21 @@
 		 * than left over from before.
 		 */
 		age = (unsigned char)(ksm_scan.seqnr - rmap_item->address);
-		BUG_ON(age > 1);
-		if (!age)
+		//BUG_ON(age > 1);
+		if (!age) {
+			/* FIXME: if rmap_item is not hugepage any more but
+			 * when it was for hugepage and inserted to unstable_tree_huge
+			 * or vice versa, this rb_erase occurs kernel BUG */
+			if(rmap_item->is_huge_page)
+				rb_erase(&rmap_item->node,
+					root_unstable_tree_huge + NUMA(rmap_item->nid));
+			else
 			rb_erase(&rmap_item->node,
 				 root_unstable_tree + NUMA(rmap_item->nid));
+		}
+		if (rmap_item->is_huge_page)
+			ksm_pages_unshared -= HPAGE_PMD_NR;
+		else
 		ksm_pages_unshared--;
 		rmap_item->address &= PAGE_MASK;
 	}
@@ -841,7 +978,7 @@
  * to the next pass of ksmd - consider, for example, how ksmd might be
  * in cmp_and_merge_page on one of the rmap_items we would be removing.
  */
-static int unmerge_ksm_pages(struct vm_area_struct *vma,
+int unmerge_ksm_pages(struct vm_area_struct *vma,
 			     unsigned long start, unsigned long end)
 {
 	unsigned long addr;
@@ -953,7 +1090,18 @@
 			}
 			cond_resched();
 		}
+		
+		while (root_stable_tree_huge[nid].rb_node) {
+			stable_node = rb_entry(root_stable_tree_huge[nid].rb_node,
+						struct stable_node, node);
+			if (remove_stable_node(stable_node)) {
+				err = -EBUSY;
+				break;	/* proceed to next nid */
+			}
+			cond_resched();
+		}
 	}
+
 	list_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {
 		if (remove_stable_node(stable_node))
 			err = -EBUSY;
@@ -1010,6 +1158,9 @@
 	/* Clean up stable nodes, but don't worry if some are still busy */
 	remove_all_stable_nodes();
 	ksm_scan.seqnr = 0;
+	//zhehua
+	nr_ksm_cows = 0;
+	nr_ksm_shares = 0;
 	return 0;
 
 error:
@@ -1030,6 +1181,231 @@
 	return checksum;
 }
 
+static int memcmp_huge_pages(struct page *page1, struct page *page2)
+{
+	if (PageTransCompound(page1) && PageTransCompound(page2)) {
+		/* Can I use get_user_huge_page? */
+		int ret;
+		char *addr1, *addr2;
+		VM_BUG_ON(!PageHead(page1));
+		VM_BUG_ON(!PageHead(page2));
+
+		addr1 = kmap_atomic(page1);
+		addr2 = kmap_atomic(page2);
+		ret = memcmp(addr1, addr2, HPAGE_PMD_SIZE);
+		kunmap_atomic(addr2);
+		kunmap_atomic(addr1);
+
+		return ret;
+	}
+
+	trace_printk("memcmp of hugepage is ignored\n");
+	return 0;
+}
+
+/* this function is called with page lock for page 1 but not for page 2 */
+int pages_identical(struct page *page1, struct page *page2)
+{
+	unsigned int i = 0;
+	if (PageTransCompound(page1) && PageTransCompound(page2)) {
+		/* Can I use get_user_huge_page? */
+		int ret;
+
+		VM_BUG_ON(!PageHead(page1));
+		VM_BUG_ON(!PageHead(page2));
+
+		/* FIXME: In case of Huge page, is it OK only to hold 
+		 * spinlock of header page? */
+		if (!trylock_page(page2))
+			return 0;
+#if 1
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			ret = memcmp_pages(page1 + i, page2 + i);
+
+			if (ret)
+				break;
+		}
+#else
+		ret = memcmp_huge_pages(page1, page2);
+#endif
+		unlock_page(page2);
+
+		return !ret;
+	} 
+	return !memcmp_pages(page1, page2);
+}
+
+/* this function is called with page lock for page 1 but not for page 2 */
+static inline int pages_identical1(struct page *page1, struct page *page2)
+{
+	unsigned int i = 0;
+	if (PageTransCompound(page1) && PageTransCompound(page2)) {
+		/* Can I use get_user_huge_page? */
+		int ret;
+
+		VM_BUG_ON(!PageHead(page1));
+		VM_BUG_ON(!PageHead(page2));
+
+		/* FIXME: In case of Huge page, is it OK only to hold 
+		 * spinlock of header page? */
+		if (!trylock_page(page2))
+			return 0;
+#if 1
+		for (i = 0; i < HPAGE_PMD_NR; i++) {
+			ret = memcmp_pages(page1 + i, page2 + i);
+
+			if (ret)
+				break;
+		}
+#else
+		ret = memcmp_huge_pages(page1, page2);
+#endif
+		unlock_page(page2);
+
+		return !ret;
+	} 
+	return !memcmp_pages(page1, page2);
+}
+
+/* This function does not consider other architectures but for x86_64
+ * so update_mmu_cache_pmd pte_unmap.. etc is not considered */
+static int write_protect_huge_page(struct vm_area_struct *vma, 
+		struct page *page, pmd_t *orig_pmd)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	unsigned long addr;
+	pmd_t *pmdp;
+	spinlock_t *ptl;
+	int err = -EFAULT;
+	struct mmu_notifier_range range;
+
+	addr = page_address_in_vma(page, vma);
+	if (addr == -EFAULT)
+		goto out;
+
+	if (!PageTransCompound(page))
+		trace_printk("page is not a huge page\n");
+
+	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, mm, addr,
+				addr + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
+
+	/* pmd operation is serialized by spinlock */
+	pmdp = page_check_address_pmd(page, mm, addr, 
+			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
+	if (!pmdp) 
+		goto out_mn_notifier;
+
+	/* note: I think I don't need to handle the case wherein the
+	 * huge page is in swapcache since if the huge pages are reclaimed
+	 * it means the huge page is already splitted so it is not the case
+	 * for huge page sharing */
+	if (pmd_write(*pmdp) || pmd_dirty(*pmdp)){
+		pmd_t entry;
+		/* Think about the case to handle here 
+		 * Huge page is dirtied?, other cases? */
+
+		/* TLB flush */
+		entry = pmdp_huge_clear_flush_notify(vma, addr, pmdp);
+
+		if (pmd_dirty(entry))
+			set_page_dirty(page);
+
+		entry = pmd_wrprotect(entry);
+		/* why should I make it clean? it copied it from write_protect_page */
+		entry = pmd_clear_flags(entry, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);
+
+		/* finally, change write bit of page table but 
+		 * I did not optimize it with pte_change mmu notifier 
+		 * as original implementation does 
+		 * so I just invalidate the corresponding entry in NPT 
+		 * later, KVM will apply this changes during the ept violation */
+		set_pmd_at(mm, addr, pmdp, entry);
+	}
+
+	*orig_pmd = *pmdp;
+	err = 0;
+
+	//ksm_printk("va: %lx - %lx\n", mmun_start, mmun_end);
+	spin_unlock(ptl);
+out_mn_notifier:
+	mmu_notifier_invalidate_range_end(&range);
+out:
+	return err;
+}
+
+/**
+ * replace_huge_page - replace page in vma by new ksm page
+ * @vma:      vma that holds the pte pointing to page
+ * @page:     the page we are replacing by kpage
+ * @kpage:    the ksm page we replace page by
+ * @orig_pmd: the original value of the pmd for page
+ *
+ * Returns 0 on success, -EFAULT on failure.
+ */
+static int replace_huge_page(struct vm_area_struct *vma, struct page *page,
+			struct page *kpage, pmd_t orig_pmd)
+{
+	struct mm_struct *mm = vma->vm_mm;
+	pmd_t *pmdp, entry;
+	spinlock_t *ptl;
+	unsigned long addr;
+	int err = -EFAULT;
+	unsigned long mmun_start, mmun_end;	/* For mmu_notifiers */
+	struct mmu_notifier_range range;
+
+	BUG_ON(PageTransCompound(kpage) != PageTransCompound(page));
+
+	addr = page_address_in_vma(page, vma);
+	if (addr == -EFAULT)
+		goto out;
+
+	addr &= HPAGE_PMD_MASK;
+
+	mmun_start = addr;
+	mmun_end = addr + HPAGE_PMD_SIZE;
+	mmu_notifier_range_init(&range, MMU_NOTIFY_CLEAR, 0, vma, mm, addr,
+		addr + HPAGE_PMD_SIZE);
+	mmu_notifier_invalidate_range_start(&range);
+
+	/* pmd operation is serialized by spinlock */
+	pmdp = page_check_address_pmd(page, mm, addr, 
+			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &ptl);
+	if (!pmdp) 
+		goto out_mn_notifier;
+	ksm_printk("0: %lx - %lx\n", mmun_start, mmun_end);
+
+	if (!pmd_same(*pmdp, orig_pmd)) {
+		//trace_printk("%s: pmd value does not match\n", __func__);
+		spin_unlock(ptl);
+		goto out_mn_notifier;
+	}
+
+	get_page(kpage);
+	page_add_anon_rmap(kpage, vma, addr, true);
+	pmdp_huge_clear_flush_notify(vma, addr, pmdp);
+
+	/* mk_huge_pmd */
+	entry = mk_pmd(kpage, vma->vm_page_prot);
+	entry = pmd_mkhuge(entry);
+	/* finally, change mapping of page's pmd to kpage's pmd */
+	set_pmd_at(mm, addr, pmdp, entry);
+
+	page_remove_rmap(page, true);
+	if (!page_mapped(page))
+		try_to_free_swap(page);
+	put_page(page);
+
+	spin_unlock(ptl);
+	err = 0;
+
+	ksm_printk("1: %lx - %lx\n", mmun_start, mmun_end);
+out_mn_notifier:
+	mmu_notifier_invalidate_range_end(&range);
+out:
+	return err;
+}
+
 static int write_protect_page(struct vm_area_struct *vma, struct page *page,
 			      pte_t *orig_pte)
 {
@@ -1191,6 +1567,82 @@
 }
 
 /*
+ * try_to_merge_huge_page - take two pages and merge them into one
+ * @vma: the vma that holds the pte pointing to page
+ * @page: the PageAnon page that we want to replace with kpage
+ * @kpage: the PageKsm page that we want to map instead of page,
+ *         or NULL the first time when we want to use page as kpage.
+ *
+ * This function returns 0 if the pages were merged, -EFAULT otherwise.
+ */
+static int try_to_merge_huge_page(struct vm_area_struct *vma,
+				 struct page *page, struct page *kpage)
+{
+	pmd_t orig_pmd  = __pmd(0);
+	int err = -EFAULT;
+
+	if (page == kpage)			/* ksm page forked */
+		return 0;
+
+	if (!(vma->vm_flags & VM_MERGEABLE))
+		goto out;
+	if (!PageAnon(page))
+		goto out;
+	if (kpage && !PageTransCompound(kpage))
+		goto out;
+
+	BUG_ON(!PageTransCompound(page));
+
+	BUG_ON(page != compound_head(page));
+
+	/*
+	 * We need the page lock to read a stable PageSwapCache in
+	 * write_protect_page().  We use trylock_page() instead of
+	 * lock_page() because we don't want to wait here - we
+	 * prefer to continue scanning and merging different pages,
+	 * then come back to this page when it is unlocked.
+	 */
+	if (!trylock_page(page))
+		goto out;
+
+	/*
+	 * If this anonymous page is mapped only here, its pte may need
+	 * to be write-protected.  If it's mapped elsewhere, all of its
+	 * ptes are necessarily already write-protected.  But in either
+	 * case, we need to lock and check page_count is not raised.
+	 */
+	if (write_protect_huge_page(vma, page, &orig_pmd) == 0) {
+		if (!kpage) {
+			/*
+			 * While we hold page lock, upgrade page from
+			 * PageAnon+anon_vma to PageKsm+NULL stable_node:
+			 * stable_tree_insert() will update stable_node.
+			 */
+			set_page_stable_node(page, NULL);
+			mark_page_accessed(page);
+			err = 0;
+		} else if (pages_identical(page, kpage)) {
+			err = replace_huge_page(vma, page, kpage, orig_pmd);
+		}
+	}
+
+	if ((vma->vm_flags & VM_LOCKED) && kpage && !err) {
+		munlock_vma_page(page);
+		if (!PageMlocked(kpage)) {
+			unlock_page(page);
+			lock_page(kpage);
+			mlock_vma_page(kpage);
+			page = kpage;		// for final unlock 
+		}
+	}
+
+	unlock_page(page);
+out:
+	BUG_ON(pmd_write(orig_pmd));
+	return err;
+}
+
+/*
  * try_to_merge_one_page - take two pages and merge them into one
  * @vma: the vma that holds the pte pointing to page
  * @page: the PageAnon page that we want to replace with kpage
@@ -1286,7 +1738,12 @@
 	if (!vma)
 		goto out;
 
+	if (rmap_item->is_huge_page && page &&
+			page_trans_compound_anon(page)) {
+		err = try_to_merge_huge_page(vma, page, kpage);
+	} else {
 	err = try_to_merge_one_page(vma, page, kpage);
+	}
 	if (err)
 		goto out;
 
@@ -1549,7 +2006,8 @@
  * This function returns the stable tree node of identical content if found,
  * NULL otherwise.
  */
-static struct page *stable_tree_search(struct page *page)
+static struct page *stable_tree_search(struct page *page, 
+		struct rmap_item *rmap_item)
 {
 	int nid;
 	struct rb_root *root;
@@ -1557,6 +2015,9 @@
 	struct rb_node *parent;
 	struct stable_node *stable_node, *stable_node_dup, *stable_node_any;
 	struct stable_node *page_node;
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 
 	page_node = page_stable_node(page);
 	if (page_node && page_node->head != &migrate_nodes) {
@@ -1566,6 +2027,9 @@
 	}
 
 	nid = get_kpfn_nid(page_to_pfn(page));
+	if (rmap_item->is_huge_page)
+		root = root_stable_tree_huge + nid;
+	else
 	root = root_stable_tree + nid;
 again:
 	new = &root->rb_node;
@@ -1628,8 +2092,45 @@
 			 */
 			goto again;
 		}
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) { 
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(page);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
+			ret = memcmp_pages(page, tree_page);
+		}
+#else
+		if (rmap_item->is_huge_page) { 
+			int i, r, match;
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(page + i, tree_page + i);
+					if (!r)
+						match++;
+				}
 
+				if (match > 0 && match < HPAGE_PMD_NR)
+					trace_printk("%lx %d\n", page_to_pfn(page), match);
+			}
+
+			ret = memcmp_huge_pages(page, tree_page);
+		} else {
 		ret = memcmp_pages(page, tree_page);
+		}
+#endif
+
 		put_page(tree_page);
 
 		parent = *new;
@@ -1801,7 +2302,8 @@
  * This function returns the stable tree node just allocated on success,
  * NULL otherwise.
  */
-static struct stable_node *stable_tree_insert(struct page *kpage)
+static struct stable_node *stable_tree_insert(struct page *kpage, 
+		struct rmap_item *rmap_item)
 {
 	int nid;
 	unsigned long kpfn;
@@ -1810,10 +2312,16 @@
 	struct rb_node *parent;
 	struct stable_node *stable_node, *stable_node_dup, *stable_node_any;
 	bool need_chain = false;
-
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 	kpfn = page_to_pfn(kpage);
 	nid = get_kpfn_nid(kpfn);
+	if (rmap_item->is_huge_page)
+		root = root_stable_tree_huge + nid;
+	else
 	root = root_stable_tree + nid;
+
 again:
 	parent = NULL;
 	new = &root->rb_node;
@@ -1864,7 +2372,45 @@
 			goto again;
 		}
 
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) {
+			if (PageTransCompound(tree_page) != PageTransCompound(kpage))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(kpage);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
 		ret = memcmp_pages(kpage, tree_page);
+		}
+#else
+		if (rmap_item->is_huge_page) {
+			int i, r, match;
+			if (PageTransCompound(kpage) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(tree_page + i, kpage + i);
+					if (!r)
+						match++;
+				}
+
+				if (match > 0 && match < HPAGE_PMD_NR)
+					ksm_printk("%lx %d\n", page_to_pfn(tree_page), match);
+			}
+
+			ret = memcmp_huge_pages(kpage, tree_page);
+		} else {
+			ret = memcmp_pages(kpage, tree_page);
+		}
+#endif
+
 		put_page(tree_page);
 
 		parent = *new;
@@ -1882,6 +2428,11 @@
 	if (!stable_node_dup)
 		return NULL;
 
+	if (rmap_item->is_huge_page)
+		stable_node_dup->is_huge_kpage = 1;
+	else
+		stable_node_dup->is_huge_kpage = 0;
+
 	INIT_HLIST_HEAD(&stable_node_dup->hlist);
 	stable_node_dup->kpfn = kpfn;
 	set_page_stable_node(kpage, stable_node_dup);
@@ -1929,8 +2480,14 @@
 	struct rb_root *root;
 	struct rb_node *parent = NULL;
 	int nid;
+#ifdef USE_JHASH
+	u32 digest1, digest2;
+#endif
 
 	nid = get_kpfn_nid(page_to_pfn(page));
+	if (rmap_item->is_huge_page)
+		root = root_unstable_tree_huge + nid;
+	else
 	root = root_unstable_tree + nid;
 	new = &root->rb_node;
 
@@ -1953,8 +2510,44 @@
 			return NULL;
 		}
 
+#ifdef USE_JHASH
+		if (rmap_item->is_huge_page) { 
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+			digest1 = get_page_hash_digest(page);
+			digest2 = get_page_hash_digest(tree_page);
+
+			if (digest1 > digest2)
+				ret = 1;
+			else if (digest1 < digest2)
+				ret = -1;
+			else
+				ret = 0;
+		} else {
 		ret = memcmp_pages(page, tree_page);
+		}
+#else
+		if (rmap_item->is_huge_page) {
+			int i, r, match;
+			if (PageTransCompound(page) != PageTransCompound(tree_page))
+				trace_printk("%s: page table is not matched\n", __func__);
+
+			if (ksm_debug == 2) {
+				for (i = 0, match = 0; i < HPAGE_PMD_NR; i++) {
+					r = memcmp_pages(page + i, tree_page + i);
+					if (!r)
+						match++;
+				}
 
+				if (match > 0 && match < HPAGE_PMD_NR)
+					ksm_printk("%lx %d\n", page_to_pfn(page), match);
+			}
+
+			ret = memcmp_huge_pages(page, tree_page);
+		} else {
+			ret = memcmp_pages(page, tree_page);
+		}
+#endif
 		parent = *new;
 		if (ret < 0) {
 			put_page(tree_page);
@@ -1982,7 +2575,9 @@
 	DO_NUMA(rmap_item->nid = nid);
 	rb_link_node(&rmap_item->node, parent, new);
 	rb_insert_color(&rmap_item->node, root);
-
+	if (rmap_item->is_huge_page > 0)
+		ksm_pages_unshared += HPAGE_PMD_NR;
+	else
 	ksm_pages_unshared++;
 	return NULL;
 }
@@ -2018,11 +2613,29 @@
 	rmap_item->address |= STABLE_FLAG;
 	hlist_add_head(&rmap_item->hlist, &stable_node->hlist);
 
-	if (rmap_item->hlist.next)
+	//zhehua
+	nr_ksm_shares++;
+
+	if (rmap_item->is_huge_page != stable_node->is_huge_kpage) {
+		trace_printk("%s: type mismatch %d %d\n",
+				__func__, rmap_item->is_huge_page, 
+				stable_node->is_huge_kpage);
+	}
+
+	if (rmap_item->hlist.next) {
+		if (rmap_item->is_huge_page > 0) {
+			ksm_pages_sharing += HPAGE_PMD_NR;
+			ksm_pages_sharing_huge += HPAGE_PMD_NR;
+			ksm_pages_sharing_huge1 += HPAGE_PMD_NR;
+		} else
 		ksm_pages_sharing++;
+	} else {
+		if (rmap_item->is_huge_page > 0)
+			ksm_pages_shared += HPAGE_PMD_NR;
 	else
 		ksm_pages_shared++;
 }
+}
 
 /*
  * cmp_and_merge_page - first see if page can be merged into the stable tree;
@@ -2049,6 +2662,10 @@
 		if (stable_node->head != &migrate_nodes &&
 		    get_kpfn_nid(READ_ONCE(stable_node->kpfn)) !=
 		    NUMA(stable_node->nid)) {
+			if (rmap_item->is_huge_page)
+				rb_erase(&stable_node->node,
+						root_stable_tree_huge + NUMA(stable_node->nid));
+			else
 			stable_node_dup_del(stable_node);
 			stable_node->head = &migrate_nodes;
 			list_add(&stable_node->list, stable_node->head);
@@ -2065,7 +2682,7 @@
 	}
 
 	/* We first start with searching the page inside the stable tree */
-	kpage = stable_tree_search(page);
+	kpage = stable_tree_search(page, rmap_item);
 	if (kpage == page && rmap_item->head == stable_node) {
 		put_page(kpage);
 		return;
@@ -2074,6 +2691,12 @@
 	remove_rmap_item_from_tree(rmap_item);
 
 	if (kpage) {
+		/* this is the case where head page of page and kpage has
+		 * identical contents but kpage or page is not huge page */
+		if ((ksm_merge_hugepage >= HPAGE_MERGE_NO_SPLIT) &&
+			(PageTransCompound(page) != PageTransCompound(kpage)))
+			goto unstable_search;
+
 		if (PTR_ERR(kpage) == -EBUSY)
 			return;
 
@@ -2091,7 +2714,7 @@
 		put_page(kpage);
 		return;
 	}
-
+unstable_search:
 	/*
 	 * If the hash value of the page has changed from the last time
 	 * we calculated it, this page is changing frequently: therefore we
@@ -2157,7 +2780,7 @@
 			 * node in the stable tree and add both rmap_items.
 			 */
 			lock_page(kpage);
-			stable_node = stable_tree_insert(kpage);
+			stable_node = stable_tree_insert(kpage, rmap_item);
 			if (stable_node) {
 				stable_tree_append(tree_rmap_item, stable_node,
 						   false);
@@ -2218,6 +2841,7 @@
 		rmap_item->address = addr;
 		rmap_item->rmap_list = *rmap_list;
 		*rmap_list = rmap_item;
+		rmap_item->is_huge_page = -1;
 	}
 	return rmap_item;
 }
@@ -2229,6 +2853,10 @@
 	struct vm_area_struct *vma;
 	struct rmap_item *rmap_item;
 	int nid;
+	unsigned long mm_rss;
+	unsigned long count_hpage;
+	struct mem_cgroup *memcg;
+	unsigned long vm_flags;
 
 	if (list_empty(&ksm_mm_head.mm_list))
 		return NULL;
@@ -2267,8 +2895,10 @@
 			}
 		}
 
-		for (nid = 0; nid < ksm_nr_node_ids; nid++)
+		for (nid = 0; nid < ksm_nr_node_ids; nid++){
 			root_unstable_tree[nid] = RB_ROOT;
+			root_unstable_tree_huge[nid] = RB_ROOT;
+		}
 
 		spin_lock(&ksm_mmlist_lock);
 		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
@@ -2309,7 +2939,13 @@
 				cond_resched();
 				continue;
 			}
-			if (PageAnon(*page)) {
+			if (PageAnon(*page)||
+			    page_trans_compound_anon(*page)) {
+
+				if (ksm_merge_hugepage == NO_HPAGE_MERGE 
+						&& page_trans_compound_anon(*page))
+					goto skip;
+
 				flush_anon_page(vma, *page, ksm_scan.address);
 				flush_dcache_page(*page);
 				rmap_item = get_next_rmap_item(slot,
@@ -2317,12 +2953,91 @@
 				if (rmap_item) {
 					ksm_scan.rmap_list =
 							&rmap_item->rmap_list;
+
+					if (page_trans_compound_anon(*page)) {
+						void *addr;
+
+						switch (ksm_merge_hugepage) {
+							// case HPAGE_MERGE_PRESERVE:
+							// 	mm_rss = get_mm_rss(rmap_item->mm);
+							// 	count_hpage = osa_get_hpage_count(rmap_item->mm);
+
+							// 	/* FIXME: subtle problem is here
+							// 	 * at this time(scanning), hpage was over this threshold 
+							// 	 * and the rmap_item is kept in unstable tree. Once ksmd
+							// 	 * splits to hpages to reach this threshold so it stops 
+							// 	 * marking as non-hugepage but items, which has been stored 
+							// 	 * in unstable tree, can still split hpage, enabling spliting
+							// 	 * more hpages */
+							// 	if (count_hpage != 0 &&
+							// 	((hpage_preserve * mm_rss / 100) < 
+							// 			 (HPAGE_PMD_NR * count_hpage))) {
+							// 		/* treat as base page */
+							// 		if (rmap_item->is_huge_page < 0)
+							// 			ksm_rmap_items++;
+
+							// 		rmap_item->is_huge_page = 0;
+							// 		ksm_scan.address += PAGE_SIZE;
+							// 		adaptive_sleep_interval = ksm_thread_sleep_millisecs;
+							// 		break;
+							// 	}
+							// 	// fall-through
+							// 	/* handle as hugepage sharing */
+							case HPAGE_MERGE_NO_SPLIT:
+								if (*page != compound_head(*page))
+									goto skip;
+								if (rmap_item->is_huge_page < 0)
+									ksm_rmap_items += HPAGE_PMD_NR;
+								rmap_item->is_huge_page = 1;
+								ksm_scan.address += HPAGE_PMD_SIZE;
+								break;
+							case HPAGE_MERGE_FREQUENCY:
+								if (PageCompound(*page)) {
+									if (*page != compound_head(*page))
+										goto skip;
+									addr = page_address(*page);
+									memcg=(*page)->mem_cgroup;
+									// frequently accessed hugepage
+									if (PageActive(*page) && page_referenced(*page, 0, memcg, &vm_flags)) {
+										if (rmap_item->is_huge_page < 0)
+											ksm_rmap_items += HPAGE_PMD_NR;
+										rmap_item->is_huge_page = 1;
+										ksm_scan.address += HPAGE_PMD_SIZE;
+									}else {
+										// if (!trylock_page(*page))
+										// 	goto skip;
+										// split_huge_page(*page);
+										// unlock_page(*page);
+										if (rmap_item->is_huge_page < 0)
+											ksm_rmap_items++;
+										rmap_item->is_huge_page = 0;
 					ksm_scan.address += PAGE_SIZE;
+									}
+								}
+								break;
+								// fall-through to split hugepage path.
+							default:
+								/* treat as base page */
+								if (rmap_item->is_huge_page < 0)
+									ksm_rmap_items++;
+								rmap_item->is_huge_page = 0;
+								ksm_scan.address += PAGE_SIZE;
+								// adaptive_sleep_interval = ksm_thread_sleep_millisecs;
+								break;
+						}
+					} else {
+						if (rmap_item->is_huge_page < 0)
+							ksm_rmap_items++;
+
+						rmap_item->is_huge_page = 0;
+						ksm_scan.address += PAGE_SIZE;
+					}
 				} else
 					put_page(*page);
 				mmap_read_unlock(mm);
 				return rmap_item;
 			}
+skip:
 			put_page(*page);
 			ksm_scan.address += PAGE_SIZE;
 			cond_resched();
@@ -2389,13 +3104,17 @@
 {
 	struct rmap_item *rmap_item;
 	struct page *page;
+	unsigned int checksum;
 
 	while (scan_npages-- && likely(!freezing(current))) {
 		cond_resched();
 		rmap_item = scan_get_next_rmap_item(&page);
 		if (!rmap_item)
 			return;
+		if (ksm_docmp) {
 		cmp_and_merge_page(page, rmap_item);
+		}
+		checksum = calc_checksum(page);
 		put_page(page);
 	}
 }
@@ -2763,7 +3482,24 @@
 				node = rb_next(node);
 			cond_resched();
 		}
+
+		node = rb_first(root_stable_tree_huge + nid);
+		while (node) {
+			stable_node = rb_entry(node, struct stable_node, node);
+			if (stable_node->kpfn >= start_pfn &&
+					stable_node->kpfn < end_pfn) {
+				/*
+				 * Don't get_ksm_page, page has already gone:
+				 * which is why we keep kpfn instead of page*
+				 */
+				remove_node_from_stable_tree(stable_node);
+				node = rb_first(root_stable_tree_huge + nid);
+			} else
+				node = rb_next(node);
+			cond_resched();
+		}
 	}
+
 	list_for_each_entry_safe(stable_node, next, &migrate_nodes, list) {
 		if (stable_node->kpfn >= start_pfn &&
 		    stable_node->kpfn < end_pfn)
@@ -2819,6 +3555,107 @@
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 
+static inline int is_zero_page(struct page *page) 
+{
+	unsigned int count = 0;
+	int ret = 0;
+	void *addr;
+	char *mem;
+
+	if (PageTransCompound(page))
+		count = HPAGE_PMD_SIZE;
+	else
+		count = PAGE_SIZE;
+
+	addr = kmap_atomic(page);
+	mem = addr;
+
+	while (count--) {
+		if (*mem != '\0') {
+			ret = -1;
+			break;
+		}
+		mem++;
+	}
+
+	kunmap_atomic(addr);
+	return ret;
+}
+
+static inline int check_stable_tree(struct rb_root *tree, 
+		unsigned long *count, unsigned long *zero_pages)
+{
+	struct stable_node *stable_node;
+	struct rb_node *node;
+	struct hlist_node *iter, *tmp;	
+	int nid;
+
+	for (nid = 0, *count = 0, *zero_pages = 0; nid < ksm_nr_node_ids; nid++) {
+		node = rb_first(tree + nid);
+		while (node) {
+			stable_node = rb_entry(node, struct stable_node, node);
+			/* print necessary info from stable_node 
+			ksm_printk("pfn %lx hugepage %d\n", 
+					stable_node->kpfn, stable_node->is_huge_kpage);
+			*/
+
+			hlist_for_each_safe(iter, tmp, &stable_node->hlist) {
+				if (is_zero_page(pfn_to_page(stable_node->kpfn))) 
+					(*zero_pages)++;
+				(*count)++;
+			}
+
+			node = rb_next(node);
+		}
+	}
+
+	return 0;
+}
+
+static inline int check_unstable_tree(struct rb_root *tree,
+		unsigned long *count)
+{
+	int nid;
+	struct rb_node *node;
+	struct rmap_item *tree_rmap_item;
+
+	for (nid = 0, *count = 0; nid < ksm_nr_node_ids; nid++) {
+		node = rb_first(tree + nid);
+		while (node) {
+			tree_rmap_item = rb_entry(node, struct rmap_item, node);
+			/* print necessary info from rmap_item */	
+			node = rb_next(node);
+			(*count)++;
+		}
+	}
+
+	return 0;
+}
+
+/* functions for debugging */
+static int check_stable_unstable_tree(void)
+{
+	unsigned long count;
+	unsigned long zero_pages;
+
+
+	check_stable_tree(root_stable_tree, &count, &zero_pages);
+	trace_printk("stable_tree:count = %lu zero_pages = %lu\n", 
+			count, zero_pages);
+
+	check_stable_tree(root_stable_tree_huge, &count, &zero_pages);
+	trace_printk("stable_tree_huge:count = %lu zero_pages = %lu\n", 
+			count, zero_pages);
+
+	check_unstable_tree(root_unstable_tree, &count);
+	trace_printk("unstable_tree:count = %lu\n", count);
+
+	check_unstable_tree(root_unstable_tree_huge, &count);
+	trace_printk("unstable_tree_huge:count = %lu\n", count);
+
+	return 0;
+}
+
 #ifdef CONFIG_SYSFS
 /*
  * This all compiles without CONFIG_SYSFS, but is a waste of space.
@@ -2901,7 +3738,7 @@
 	 * breaking COW to free the pages_shared (but leaves mm_slots
 	 * on the list for when ksmd may be set running again).
 	 */
-
+	ksm_pages_sharing_huge1 = 0;
 	mutex_lock(&ksm_thread_mutex);
 	wait_while_offlining();
 	if (ksm_run != flags) {
@@ -3046,19 +3883,101 @@
 }
 KSM_ATTR(max_page_sharing);
 
+//zhehua
+static ssize_t nr_ksm_cows_show(struct kobject *kobj,
+	struct kobj_attribute *attr, char *buf)
+{
+return sprintf(buf, "%lu\n", nr_ksm_cows);
+}
+KSM_ATTR_RO(nr_ksm_cows);
+
+static ssize_t nr_ksm_shares_show(struct kobject *kobj,
+	struct kobj_attribute *attr, char *buf)
+{
+return sprintf(buf, "%lu\n", nr_ksm_shares);
+}
+KSM_ATTR_RO(nr_ksm_shares);
+
 static ssize_t pages_shared_show(struct kobject *kobj,
 				 struct kobj_attribute *attr, char *buf)
 {
 	return sprintf(buf, "%lu\n", ksm_pages_shared);
 }
-KSM_ATTR_RO(pages_shared);
+
+static ssize_t pages_shared_store(struct kobject *kobj, 
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int err;
+	unsigned long pages_shared;
+
+	err = kstrtoul(buf, 10, &pages_shared);
+	if (err || pages_shared > UINT_MAX)
+		return -EINVAL;
+
+	ksm_pages_shared = pages_shared;
+
+	return count;
+}
+KSM_ATTR(pages_shared);
+
+static ssize_t do_cmp_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", ksm_docmp);
+}
+
+static ssize_t do_cmp_store(struct kobject *kobj, 
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int err;
+	unsigned long docmp;
+
+	err = kstrtoul(buf, 10, &docmp);
+	if (err || docmp > UINT_MAX)
+		return -EINVAL;
+
+	ksm_docmp = docmp;
+
+	return count;
+}
+KSM_ATTR(do_cmp);
 
 static ssize_t pages_sharing_show(struct kobject *kobj,
 				  struct kobj_attribute *attr, char *buf)
 {
 	return sprintf(buf, "%lu\n", ksm_pages_sharing);
 }
-KSM_ATTR_RO(pages_sharing);
+
+static ssize_t pages_sharing_store(struct kobject *kobj, 
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int err;
+	unsigned long pages_sharing;
+
+	err = kstrtoul(buf, 10, &pages_sharing);
+	if (err || pages_sharing > UINT_MAX)
+		return -EINVAL;
+
+	ksm_pages_sharing = pages_sharing;
+	ksm_pages_sharing_huge = pages_sharing;
+
+	return count;
+}
+KSM_ATTR(pages_sharing);
+
+static ssize_t pages_sharing_huge_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", ksm_pages_sharing_huge);
+}
+KSM_ATTR_RO(pages_sharing_huge);
+
+static ssize_t pages_sharing_huge1_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%lu\n", ksm_pages_sharing_huge1);
+}
+KSM_ATTR_RO(pages_sharing_huge1);
 
 static ssize_t pages_unshared_show(struct kobject *kobj,
 				   struct kobj_attribute *attr, char *buf)
@@ -3084,6 +4003,41 @@
 }
 KSM_ATTR_RO(pages_volatile);
 
+static ssize_t merge_hugepage_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buf)
+{
+	switch(ksm_merge_hugepage) {
+		case NO_HPAGE_MERGE:
+			return sprintf(buf, "NO_HPAGE_MERGE\n");
+		case HPAGE_MERGE_SPLIT:
+			return sprintf(buf, "HPAGE_SPLIT\n");
+		case HPAGE_MERGE_NO_SPLIT:
+			return sprintf(buf, "HPAGE_NO_SPLIT\n");
+		case HPAGE_MERGE_PRESERVE:
+			return sprintf(buf, "HPAGE_PERSERVE\n");
+		case HPAGE_MERGE_FREQUENCY:
+			return sprintf(buf, "HPAGE_FREQUENCY\n");
+		default:
+			return sprintf(buf, "%u\n", ksm_merge_hugepage);
+	}
+}
+
+static ssize_t merge_hugepage_store(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	int err;
+	unsigned int flag;
+	err = kstrtou32(buf, 10, &flag);
+	if (err || flag > HPAGE_MERGE_MAX)
+		return -EINVAL;
+
+	ksm_merge_hugepage = flag;
+
+	return count;
+}
+KSM_ATTR(merge_hugepage);
+
 static ssize_t stable_node_dups_show(struct kobject *kobj,
 				     struct kobj_attribute *attr, char *buf)
 {
@@ -3131,15 +4085,60 @@
 }
 KSM_ATTR_RO(full_scans);
 
+/* used for debugging */
+static ssize_t debug_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "debugging command");
+}
+
+/* ksm_debug:
+ * 1 - print stable tree and unstable tree
+ * 2 - print case where KSMd fail to merge hugepage 
+ *	   due to the partial matching
+ * 3 - traverse stable tree and count the case where sharing share is 
+ *     zero page
+ */
+
+static ssize_t debug_store(struct kobject *kobj,
+		struct kobj_attribute *attr,
+		const char *buf, size_t count)
+{
+	int err;
+	unsigned int flag;
+	err = kstrtou32(buf, 10, &flag);
+
+	if (err || flag > HPAGE_MERGE_MAX)
+		return -EINVAL;
+
+	ksm_debug = flag;
+
+	switch(flag) {
+		case 1:
+			check_stable_unstable_tree();
+			break;
+		default:
+			break;
+	}
+
+	return count;
+}
+KSM_ATTR(debug);
+
 static struct attribute *ksm_attrs[] = {
+	&do_cmp_attr.attr,
 	&sleep_millisecs_attr.attr,
 	&pages_to_scan_attr.attr,
 	&run_attr.attr,
 	&pages_shared_attr.attr,
 	&pages_sharing_attr.attr,
+	&pages_sharing_huge_attr.attr,
+	&pages_sharing_huge1_attr.attr,
 	&pages_unshared_attr.attr,
 	&pages_volatile_attr.attr,
 	&full_scans_attr.attr,
+	&merge_hugepage_attr.attr,
+	&debug_attr.attr,
 #ifdef CONFIG_NUMA
 	&merge_across_nodes_attr.attr,
 #endif
@@ -3148,6 +4147,9 @@
 	&stable_node_dups_attr.attr,
 	&stable_node_chains_prune_millisecs_attr.attr,
 	&use_zero_pages_attr.attr,
+	//zhehua
+	&nr_ksm_cows_attr.attr,
+	&nr_ksm_shares_attr.attr,
 	NULL,
 };
 
diff --color -ruN -w -B '--exclude-from=.diff-exclude' linux-5.10/mm/memory.c Ingens-5.10-cow/mm/memory.c
--- linux-5.10/mm/memory.c	2020-12-13 22:41:30.000000000 +0000
+++ Ingens-5.10-cow/mm/memory.c	2025-03-23 07:51:07.856354062 +0000
@@ -2586,6 +2586,7 @@
 
 	if (likely(src)) {
 		copy_user_highpage(dst, src, addr, vma);
+		vma->cows++;
 		return true;
 	}
 
@@ -2832,6 +2833,7 @@
 							      vmf->address);
 		if (!new_page)
 			goto oom;
+		vma->cows++;
 	} else {
 		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,
 				vmf->address);
@@ -3104,8 +3106,12 @@
 		struct page *page = vmf->page;
 
 		/* PageKsm() doesn't necessarily raise the page refcount */
-		if (PageKsm(page) || page_count(page) != 1)
+		if (PageKsm(page) || page_count(page) != 1){
+			//zhehua
+			if (PageKsm(page))
+				nr_ksm_cows++;
 			goto copy;
+		}
 		if (!trylock_page(page))
 			goto copy;
 		if (PageKsm(page) || page_mapcount(page) != 1 || page_count(page) != 1) {
